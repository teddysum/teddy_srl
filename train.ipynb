{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "import os\n",
    "from teddy_srl import dataio\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import glob\n",
    "import os\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertModel\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n",
    "from tqdm import tqdm, trange\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from seqeval.metrics import f1_score\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "batch_size = 6\n",
    "\n",
    "try:\n",
    "    dir_path = os.path.dirname(os.path.abspath( __file__ ))\n",
    "except:\n",
    "    dir_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_file = '../datasets/korean_propbank/srl.span_based.train.conll'\n",
    "tst_file = '../datasets/korean_propbank/srl.span_based.test.conll'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn: 19306\n",
      "tst: 3778\n",
      "data example\n",
      "[['한국탁구가', '2000년', '시드니올림픽', '본선에', '남녀복식', '2개조씩을', '<tgt>', '파견할', '</tgt>', '수', '있게', '됐다.'], ['_', '_', '_', '_', '_', '_', '_', '파견.01', '_', '_', '_', '_'], ['B-ARG0', 'B-ARG2', 'I-ARG2', 'I-ARG2', 'B-ARG1', 'I-ARG1', 'X', 'O', 'X', 'O', 'B-AUX', 'B-AUX']]\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    trn = dataio.load_srl_data_for_bert(trn_file)\n",
    "    tst = dataio.load_srl_data_for_bert(tst_file)\n",
    "    print('trn:', len(trn))\n",
    "    print('tst:', len(tst))\n",
    "    print('data example')\n",
    "    print(trn[0])\n",
    "    \n",
    "    return trn, tst\n",
    "\n",
    "trn, tst = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tag2idx():\n",
    "    d = trn+tst\n",
    "    tags = []\n",
    "    for i in d:\n",
    "        for a in i[-1]:\n",
    "            tags.append(a)\n",
    "            \n",
    "    tags = sorted(list(set(tags)))\n",
    "    \n",
    "    tag2idx = {}\n",
    "    tag2idx['X'] = 0\n",
    "    tag2idx['O'] = 1\n",
    "    \n",
    "    for tag in tags:\n",
    "        if tag not in tag2idx:\n",
    "            tag2idx[tag] = len(tag2idx)\n",
    "            \n",
    "    with open(dir_path+'/data/tag2idx.json','w') as f:\n",
    "        json.dump(tag2idx, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "gen_tag2idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_path+'/data/tag2idx.json','r') as f:\n",
    "    tag2idx = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class for_BERT():\n",
    "    \n",
    "    def __init__(self, mode='training'):\n",
    "        self.mode = mode\n",
    "        \n",
    "        with open(dir_path+'/data/tag2idx.json','r') as f:\n",
    "            self.tag2idx = json.load(f)\n",
    "            \n",
    "        self.idx2tag = dict(zip(self.tag2idx.values(),self.tag2idx.keys()))\n",
    "        \n",
    "        # load pretrained BERT tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "        \n",
    "        # load BERT tokenizer with untokenizing frames\n",
    "        never_split_tuple = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")\n",
    "        added_never_split = []\n",
    "        added_never_split.append('<tgt>')\n",
    "        added_never_split.append('</tgt>')\n",
    "        added_never_split_tuple = tuple(added_never_split)\n",
    "        never_split_tuple += added_never_split_tuple\n",
    "        vocab_file_path = dir_path+'/data/bert-multilingual-cased-dict-add-frames'\n",
    "        self.tokenizer_with_frame = BertTokenizer(vocab_file_path, do_lower_case=False, max_len=256, never_split=never_split_tuple)\n",
    "        \n",
    "    def idx2tag(self, predictions):\n",
    "        pred_tags = [self.idx2tag[p_i] for p in predictions for p_i in p]\n",
    "        \n",
    "        # bert tokenizer and assign to the first token\n",
    "    def bert_tokenizer(self, text):\n",
    "        orig_tokens = text.split(' ')\n",
    "        bert_tokens = []\n",
    "        orig_to_tok_map = []\n",
    "        bert_tokens.append(\"[CLS]\")\n",
    "        for orig_token in orig_tokens:\n",
    "            orig_to_tok_map.append(len(bert_tokens))\n",
    "            bert_tokens.extend(self.tokenizer_with_frame.tokenize(orig_token))\n",
    "        bert_tokens.append(\"[SEP]\")\n",
    "\n",
    "        return orig_tokens, bert_tokens, orig_to_tok_map\n",
    "    \n",
    "    def convert_to_bert_input(self, input_data):\n",
    "        tokenized_texts, args = [],[]\n",
    "        orig_tok_to_maps = []\n",
    "        for i in range(len(input_data)):    \n",
    "            data = input_data[i]\n",
    "            text = ' '.join(data[0])\n",
    "            orig_tokens, bert_tokens, orig_to_tok_map = self.bert_tokenizer(text)\n",
    "            orig_tok_to_maps.append(orig_to_tok_map)\n",
    "            tokenized_texts.append(bert_tokens)\n",
    "\n",
    "            if self.mode == 'training':\n",
    "                ori_args = data[2]\n",
    "                arg_sequence = []\n",
    "                for i in range(len(bert_tokens)):\n",
    "                    if i in orig_to_tok_map:\n",
    "                        idx = orig_to_tok_map.index(i)\n",
    "                        ar = ori_args[idx]\n",
    "                        arg_sequence.append(ar)\n",
    "                    else:\n",
    "                        arg_sequence.append('X')\n",
    "                args.append(arg_sequence)\n",
    "\n",
    "        input_ids = pad_sequences([self.tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                              maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "        orig_tok_to_maps = pad_sequences(orig_tok_to_maps, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\", value=-1)\n",
    "        \n",
    "        if self.mode =='training':\n",
    "            arg_ids = pad_sequences([[self.tag2idx.get(ar) for ar in arg] for arg in args],\n",
    "                                    maxlen=MAX_LEN, value=self.tag2idx[\"X\"], padding=\"post\",\n",
    "                                    dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "        attention_masks = [[float(i>0) for i in ii] for ii in input_ids]    \n",
    "        data_inputs = torch.tensor(input_ids)\n",
    "        data_orig_tok_to_maps = torch.tensor(orig_tok_to_maps)\n",
    "        data_masks = torch.tensor(attention_masks)\n",
    "        \n",
    "        if self.mode == 'training':\n",
    "            data_args = torch.tensor(arg_ids)\n",
    "            bert_inputs = TensorDataset(data_inputs, data_orig_tok_to_maps, data_args, data_masks)\n",
    "        else:\n",
    "            bert_inputs = TensorDataset(data_inputs, data_orig_tok_to_maps, data_masks)\n",
    "        return bert_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_io = for_BERT(mode='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = bert_io.convert_to_bert_input(trn)\n",
    "tst_data = bert_io.convert_to_bert_input(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_path):\n",
    "    print('your model would be saved at', model_path)\n",
    "    \n",
    "    model = BertForTokenClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(bert_io.tag2idx))\n",
    "    model.to(device);\n",
    "    \n",
    "    trn_data = bert_io.convert_to_bert_input(trn)\n",
    "    sampler = RandomSampler(trn_data)\n",
    "    trn_dataloader = DataLoader(trn_data, sampler=sampler, batch_size=batch_size)\n",
    "    \n",
    "    # load optimizer\n",
    "    FULL_FINETUNING = True\n",
    "    if FULL_FINETUNING:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        param_optimizer = list(model.classifier.named_parameters()) \n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "    optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "    \n",
    "    \n",
    "    # train \n",
    "    epochs = 20\n",
    "    max_grad_norm = 1.0\n",
    "    num_of_epoch = 0\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        # TRAIN loop\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(trn_dataloader):\n",
    "            # add batch to gpu\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_orig_tok_to_maps, b_input_args, b_input_masks = batch            \n",
    "            # forward pass\n",
    "            loss = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_masks, labels=b_input_args)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # track train loss\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "#             break\n",
    "#         break\n",
    "\n",
    "        # print train loss per epoch\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "        model_saved_path = model_path+'ko-srl-epoch-'+str(num_of_epoch)+'.pt'        \n",
    "        torch.save(model, model_saved_path)\n",
    "        num_of_epoch += 1\n",
    "        \n",
    "        \n",
    "    print('...training is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your model would be saved at /disk/teddysum/models/ko_srl/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.5947883129119873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   5%|▌         | 1/20 [00:03<01:14,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.170109748840332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|█         | 2/20 [00:08<01:12,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.6381335258483887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  15%|█▌        | 3/20 [00:11<01:06,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.2494077682495117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 4/20 [00:15<01:03,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.7290772199630737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  25%|██▌       | 5/20 [00:20<01:03,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.636440634727478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|███       | 6/20 [00:24<00:56,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.607418179512024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  35%|███▌      | 7/20 [00:28<00:51,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.3993594646453857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 8/20 [00:32<00:48,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.448268175125122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  45%|████▌     | 9/20 [00:36<00:44,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.471469759941101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 10/20 [00:40<00:41,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.3021154403686523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  55%|█████▌    | 11/20 [00:41<00:28,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2221046686172485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████    | 12/20 [00:42<00:19,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2924177646636963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  65%|██████▌   | 13/20 [00:43<00:13,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1316410303115845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|███████   | 14/20 [00:43<00:09,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.3682905435562134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  75%|███████▌  | 15/20 [00:44<00:06,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1522966623306274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████  | 16/20 [00:45<00:04,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1029623746871948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  85%|████████▌ | 17/20 [00:46<00:03,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9413691759109497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|█████████ | 18/20 [00:47<00:01,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.0406194925308228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  95%|█████████▌| 19/20 [00:47<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8483509421348572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 20/20 [00:48<00:00,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...training is done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = '/disk/teddysum/models/ko_srl/'\n",
    "train(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def test(model_path):\n",
    "    models = glob.glob(model_path+'*.pt')\n",
    "    \n",
    "    result_path = './eval_result/'\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    results = []\n",
    "    \n",
    "    for m in models:\n",
    "        print('model:', m)\n",
    "        model = torch.load(m)\n",
    "        model.eval()\n",
    "        \n",
    "        tst_data = bert_io.convert_to_bert_input(tst)\n",
    "        sampler = RandomSampler(tst_data)\n",
    "        tst_dataloader = DataLoader(tst_data, sampler=sampler, batch_size=batch_size)\n",
    "        \n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        \n",
    "        pred_args, true_args = [],[]\n",
    "        for batch in tst_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_orig_tok_to_maps, b_input_args, b_input_masks = batch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                              attention_mask=b_input_masks, labels=b_input_args)\n",
    "                logits = model(b_input_ids, token_type_ids=None,\n",
    "                               attention_mask=b_input_masks)\n",
    "                \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            \n",
    "            b_pred_args = [list(p) for p in np.argmax(logits, axis=2)]\n",
    "            b_true_args = b_input_args.to('cpu').numpy().tolist()\n",
    "            \n",
    "            \n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            \n",
    "            nb_eval_examples += b_input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "            \n",
    "            for b_idx in range(len(b_true_args)):\n",
    "                \n",
    "                input_id = b_input_ids[b_idx]\n",
    "                orig_tok_to_map = b_input_orig_tok_to_maps[b_idx]                \n",
    "                pred_arg_bert = b_pred_args[b_idx]\n",
    "                true_arg_bert = b_true_args[b_idx]\n",
    "\n",
    "                pred_arg, true_arg = [],[]\n",
    "                for tok_idx in orig_tok_to_map:\n",
    "                    if tok_idx != -1:\n",
    "                        tok_id = int(input_id[tok_idx])\n",
    "                        if tok_id == 1:\n",
    "                            pass\n",
    "                        elif tok_id == 2:\n",
    "                            pass\n",
    "                        else:\n",
    "                            pred_arg.append(pred_arg_bert[tok_idx])\n",
    "                            true_arg.append(true_arg_bert[tok_idx])\n",
    "                            \n",
    "                pred_args.append(pred_arg)\n",
    "                true_args.append(true_arg) \n",
    "            \n",
    "#             break\n",
    "\n",
    "        \n",
    "        pred_arg_tags_old = [[bert_io.idx2tag[p_i] for p_i in p] for p in pred_args]\n",
    "        \n",
    "        pred_arg_tags = []\n",
    "        for old in pred_arg_tags_old:\n",
    "            new = []\n",
    "            for t in old:\n",
    "                if t == 'X':\n",
    "                    new_t = 'O'\n",
    "                else:\n",
    "                    new_t = t\n",
    "                new.append(new_t)\n",
    "            pred_arg_tags.append(new)\n",
    "            \n",
    "        valid_arg_tags = [[bert_io.idx2tag[v_i] for v_i in v] for v in true_args]\n",
    "        f1 = f1_score(pred_arg_tags, valid_arg_tags)\n",
    "                \n",
    "        print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "        print(\"Validation F1-Score: {}\".format(f1_score(pred_arg_tags, valid_arg_tags)))\n",
    "                \n",
    "        result =  m+'\\targid:'+str(f1)+'\\n'\n",
    "        results.append(result)\n",
    "        \n",
    "        epoch = m.split('-')[-1].split('.')[0]\n",
    "        fname = result_path+str(epoch)+'-result.txt'\n",
    "        \n",
    "        with open(fname, 'w') as f:\n",
    "            line = result\n",
    "            f.write(line)\n",
    "            line = 'gold'+'\\t'+'pred'+'\\n'\n",
    "            f.write(line)\n",
    "            \n",
    "            for r in range(len(pred_arg_tags)):\n",
    "                line = str(valid_arg_tags[r]) + '\\t' + str(pred_arg_tags[r])+'\\n'\n",
    "                f.write(line)\n",
    "                \n",
    "    fname = result_path+'eval_result.txt'\n",
    "    with open(fname, 'w') as f:\n",
    "        for r in results:\n",
    "            f.write(r)\n",
    "            \n",
    "    print('result is written to',fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-13.pt\n",
      "Validation loss: 1.0115739107131958\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-0.pt\n",
      "Validation loss: 3.130679130554199\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-15.pt\n",
      "Validation loss: 1.0787105560302734\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-11.pt\n",
      "Validation loss: 1.0851402282714844\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-19.pt\n",
      "Validation loss: 0.7132990956306458\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-17.pt\n",
      "Validation loss: 1.062798023223877\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-14.pt\n",
      "Validation loss: 1.237493872642517\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-18.pt\n",
      "Validation loss: 0.7079179883003235\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-10.pt\n",
      "Validation loss: 1.1703652143478394\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-3.pt\n",
      "Validation loss: 1.8746845722198486\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-5.pt\n",
      "Validation loss: 1.4824737310409546\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-16.pt\n",
      "Validation loss: 1.0080642700195312\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-1.pt\n",
      "Validation loss: 2.6055350303649902\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-6.pt\n",
      "Validation loss: 1.4613354206085205\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-4.pt\n",
      "Validation loss: 1.475710153579712\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-9.pt\n",
      "Validation loss: 0.9603704810142517\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-8.pt\n",
      "Validation loss: 1.2866287231445312\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-2.pt\n",
      "Validation loss: 2.197047233581543\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-12.pt\n",
      "Validation loss: 1.2338144779205322\n",
      "Validation F1-Score: 0\n",
      "model: /disk/teddysum/models/ko_srl/ko-srl-epoch-7.pt\n",
      "Validation loss: 1.1330419778823853\n",
      "Validation F1-Score: 0\n",
      "result is written to ./eval_result/eval_result.txt\n"
     ]
    }
   ],
   "source": [
    "model_path = '/disk/teddysum/models/ko_srl/'\n",
    "test(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
